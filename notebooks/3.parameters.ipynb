{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX 1\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIX 2\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "# import os; os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a8e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter count, FLOPs, and throughput measurement script\n",
    "\"\"\"\n",
    "measure_lggf_metrics.py\n",
    "\n",
    "Compute parameter count, FLOPs, and inference throughput (images/sec) for saved RetinaNet checkpoints\n",
    "trained with baseline, SE, CBAM, or LGF blocks.\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "1) Single checkpoint on GPU with synthetic timing images of size 640x640\n",
    "   python measure_lggf_metrics.py --ckpts /nas.dbms/asera/NEW/4.1.2/BEST_coco_nw_C3_lgf_gated_spatial_s2025_epoch_70_map_0.3267_apsmall_0.2399.pth\n",
    "\n",
    "2) Multiple checkpoints and write CSV\n",
    "   python measure_lggf_metrics.py --ckpts ckpts/*.pth --report-csv report.csv\n",
    "\n",
    "3) Force insert level if experiment name is not standard\n",
    "   python measure_lggf_metrics.py --ckpts model.pth --insert-level C3\n",
    "\n",
    "Notes:\n",
    "- FLOPs are computed with fvcore if available, otherwise thop. Both are optional.\n",
    "- Throughput is measured with random images unless you provide a directory of images (see --image-folder).\n",
    "- Checkpoints saved by the training script contain 'ema_state_dict' and 'config'; this script uses those.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %pip install -q fvcore iopath\n",
    "# # or\n",
    "# %pip install -q thop\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.argv = [\n",
    "    \"measure_lggf_metrics.py\",\n",
    "    \"--ckpts\",\n",
    "    \"/nas.dbms/asera/NEW/4.1.2/BEST_coco_nw_C3_baseline_s2025_epoch_75_map_0.3363_apsmall_2199.pth\",\n",
    "    \"/nas.dbms/asera/NEW/4.1.2/BEST_coco_nw_C3_se_s2025_epoch_70_map_0.3220_apsmall_0.2156.pth\",\n",
    "    \"/nas.dbms/asera/NEW/4.1.2/BEST_coco_nw_C3_cbam_s42_epoch_80_map_0.3422_apsmall_0.2223.pth\",\n",
    "    \"/nas.dbms/asera/NEW/4.1.2/BEST_coco_nw_C3_lgf_gated_spatial_s2025_epoch_70_map_0.3267_apsmall_0.2399.pth\",\n",
    "    \"--img-size\", \"640\",\n",
    "    \"--batch-size\", \"16\",\n",
    "    \"--measure-images\", \"1000\",\n",
    "    \"--report-csv\", \"lggf_report.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import copy\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
    "from torchvision.ops.misc import FrozenBatchNorm2d\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "# ------------------------------ Utilities -------------------------------------\n",
    "\n",
    "def try_import(module_name):\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def set_torch_determinism():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if hasattr(torch, \"use_deterministic_algorithms\"):\n",
    "        try:\n",
    "            torch.use_deterministic_algorithms(True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ------------------------------ Blocks ----------------------------------------\n",
    "\n",
    "class StableLGFBlock(nn.Module):\n",
    "    def __init__(self, channels, branches=(\"local\",\"global\"), gating_type=\"sum\", norm_groups=32, squeeze_ratio=16):\n",
    "        super().__init__()\n",
    "        self.branches = tuple(branches)\n",
    "        self.gating_type = gating_type\n",
    "        self.num_branches = len(self.branches)\n",
    "        def GN(c): return nn.GroupNorm(norm_groups, c)\n",
    "\n",
    "        # Local branch\n",
    "        self.local = None\n",
    "        if \"local\" in self.branches:\n",
    "            self.local = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, 3, padding=1, groups=channels, bias=False),\n",
    "                nn.Conv2d(channels, channels, 1, bias=False),\n",
    "                GN(channels), nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Global branch\n",
    "        self.global_branch = None\n",
    "        if \"global\" in self.branches:\n",
    "            self.global_branch = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Conv2d(channels, channels, 1, bias=False),\n",
    "                GN(channels), nn.SiLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Gates\n",
    "        if self.num_branches > 1:\n",
    "            if self.gating_type == \"softmax\":\n",
    "                self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "                self.branch_weights = nn.Parameter(torch.ones(self.num_branches))\n",
    "            elif self.gating_type == \"gated\":\n",
    "                hid = max(channels // squeeze_ratio, 4)\n",
    "                self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "                self.gate_mlp = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "                    nn.Linear(channels, hid), nn.ReLU(inplace=True),\n",
    "                    nn.Linear(hid, self.num_branches)\n",
    "                )\n",
    "                nn.init.zeros_(self.gate_mlp[-1].weight); nn.init.zeros_(self.gate_mlp[-1].bias)\n",
    "            elif self.gating_type == \"gated_spatial\":\n",
    "                r = 4\n",
    "                self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "                self.gate_reduce = nn.Conv2d(channels, channels//r, 1, bias=False)\n",
    "                self.gate_expand = nn.Conv2d(channels//r, 2*channels, 1, bias=True)\n",
    "                self.gate_norm   = nn.GroupNorm(num_groups=norm_groups, num_channels=2*channels)\n",
    "                nn.init.zeros_(self.gate_expand.weight); nn.init.zeros_(self.gate_expand.bias)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def _broadcast(self, s, like):\n",
    "        if s.dim() == 1:\n",
    "            s = s.view(-1,1,1,1)\n",
    "        return s.expand_as(like)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        L = self.local(x) if self.local is not None else None\n",
    "        if L is not None: feats.append(L)\n",
    "        G = None\n",
    "        if self.global_branch is not None:\n",
    "            g = self.global_branch(x)\n",
    "            G = g.expand_as(x)\n",
    "            feats.append(G)\n",
    "\n",
    "        if len(feats) == 1:\n",
    "            out = feats[0]\n",
    "        else:\n",
    "            if self.gating_type == \"sum\":\n",
    "                out = feats[0] + feats[1]\n",
    "                # equally weighted, no learnable params here\n",
    "            elif self.gating_type == \"softmax\":\n",
    "                tau = F.softplus(self.temperature) + 1e-3\n",
    "                w = F.softmax(self.branch_weights / tau, dim=0)\n",
    "                out = w[0]*feats[0] + w[1]*feats[1]\n",
    "            elif self.gating_type == \"gated\":\n",
    "                logits = self.gate_mlp(x.float())\n",
    "                tau = F.softplus(self.temperature.float()) + 1e-3\n",
    "                logits = logits.clamp_(-15,15)\n",
    "                w = torch.sigmoid(logits / tau).to(dtype=x.dtype)\n",
    "                wL = self._broadcast(w[:,0], L); wG = self._broadcast(w[:,1], G)\n",
    "                out = wL*L + wG*G\n",
    "            elif self.gating_type == \"gated_spatial\":\n",
    "                h = F.relu(self.gate_reduce(x), inplace=True)\n",
    "                logits = self.gate_expand(h)\n",
    "                logits = self.gate_norm(logits)\n",
    "                logits32 = logits.float().clamp_(-15,15)\n",
    "                tau = F.softplus(self.temperature.float()) + 1e-3\n",
    "                w = torch.sigmoid(logits32 / tau).to(dtype=x.dtype)\n",
    "                N, twoC, H, W = w.shape\n",
    "                C = twoC//2\n",
    "                w = w.view(N,2,C,H,W)\n",
    "                wL, wG = w[:,0], w[:,1]\n",
    "                out = wL*L + wG*G\n",
    "            else:\n",
    "                out = feats[0] + feats[1]\n",
    "\n",
    "        out = x + self.gamma*out\n",
    "        return out\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc  = nn.Sequential(\n",
    "            nn.Linear(channels, channels//reduction, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels//reduction, channels, bias=False), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b,c,_,_ = x.shape\n",
    "        y = self.fc(self.avg(x).view(b,c)).view(b,c,1,1)\n",
    "        return x * y\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, k=5, beta=20.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        # shared MLP for channel attention\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigc = nn.Sigmoid()\n",
    "        # spatial attention\n",
    "        self.convs = nn.Conv2d(2, 1, k, padding=k // 2, bias=False)\n",
    "        self.sigs = nn.Sigmoid()\n",
    "\n",
    "    def _softmax_pool_spatial(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.view(B, C, H * W)\n",
    "        x_norm = x_flat - x_flat.max(dim=2, keepdim=True).values\n",
    "        w = F.softmax(self.beta * x_norm, dim=2)\n",
    "        pooled = (w * x_flat).sum(dim=2)\n",
    "        return pooled.view(B, C, 1, 1)\n",
    "\n",
    "    def _softmax_pool_channel(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_hw_c = x.permute(0, 2, 3, 1)\n",
    "        x_norm = x_hw_c - x_hw_c.max(dim=3, keepdim=True).values\n",
    "        w = F.softmax(self.beta * x_norm, dim=3)\n",
    "        pooled = (w * x_hw_c).sum(dim=3, keepdim=True)\n",
    "        return pooled.permute(0, 3, 1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        avg_pool = self.avg(x)\n",
    "        smx_pool = self._softmax_pool_spatial(x)\n",
    "        ca = self.fc(avg_pool) + self.fc(smx_pool)\n",
    "        x = x * self.sigc(ca)\n",
    "\n",
    "        # Spatial attention\n",
    "        s_mean = x.mean(dim=1, keepdim=True)\n",
    "        s_softmax_max = self._softmax_pool_channel(x)\n",
    "        s = torch.cat([s_mean, s_softmax_max], dim=1)\n",
    "        sa = self.sigs(self.convs(s))\n",
    "        return x * sa\n",
    "\n",
    "# ----------------------- Backbones and model build ----------------------------\n",
    "\n",
    "LEVEL_MAP = {\"C3\":\"layer2\", \"C4\":\"layer3\", \"C5\":\"layer4\"}\n",
    "\n",
    "def convert_bn_to_gn(module, num_groups=32, convert_frozen=False):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.BatchNorm2d):\n",
    "            setattr(module, name, nn.GroupNorm(num_groups=num_groups, num_channels=child.num_features))\n",
    "        elif convert_frozen and isinstance(child, FrozenBatchNorm2d):\n",
    "            setattr(module, name, nn.GroupNorm(num_groups=num_groups, num_channels=child.num_features))\n",
    "        else:\n",
    "            convert_bn_to_gn(child, num_groups=num_groups, convert_frozen=convert_frozen)\n",
    "    return module\n",
    "\n",
    "def _probe_feature_names(backbone: nn.Module) -> list:\n",
    "    with torch.no_grad():\n",
    "        x = torch.zeros(1,3,224,224, device=next(backbone.parameters()).device)\n",
    "        feats = backbone(x)\n",
    "        from collections import OrderedDict\n",
    "        if not isinstance(feats, OrderedDict):\n",
    "            raise RuntimeError(\"Backbone must return OrderedDict of features\")\n",
    "        return list(feats.keys())\n",
    "\n",
    "_SIZE_MAP = {\n",
    "    \"0\": (32,48,64),\n",
    "    \"1\": (64,96,128),\n",
    "    \"2\": (128,192,256),\n",
    "    \"3\": (256,384,512),\n",
    "    \"p6\": (256,384,512),\n",
    "    \"pool\": (384,512,640),\n",
    "    \"p7\": (384,512,640),\n",
    "}\n",
    "\n",
    "def make_anchor_generator_for(backbone: nn.Module) -> AnchorGenerator:\n",
    "    names = _probe_feature_names(backbone)\n",
    "    try:\n",
    "        sizes = tuple(_SIZE_MAP[n] for n in names)\n",
    "    except KeyError as e:\n",
    "        raise RuntimeError(f\"No anchor size tuple for feature '{e.args[0]}'. Backbone keys={names}\")\n",
    "    ratios = ((0.5,1.0,2.0),) * len(sizes)\n",
    "    return AnchorGenerator(sizes=sizes, aspect_ratios=ratios)\n",
    "\n",
    "def get_block(channels, block_type, branches, gating_type):\n",
    "    if block_type == \"lgf\":\n",
    "        return StableLGFBlock(channels, branches=branches, gating_type=gating_type)\n",
    "    if block_type == \"se\":\n",
    "        return SEBlock(channels)\n",
    "    if block_type == \"cbam\":\n",
    "        return CBAMBlock(channels)\n",
    "    return nn.Identity()\n",
    "\n",
    "class CustomBackboneBlockBeforeFPN(nn.Module):\n",
    "    def __init__(self, backbone_with_fpn, selected_levels, config):\n",
    "        super().__init__()\n",
    "        self.body = backbone_with_fpn.body\n",
    "        self.fpn  = backbone_with_fpn.fpn\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = self.body(torch.zeros(1,3,224,224))\n",
    "            fpn_feats = self.fpn(feats)\n",
    "        actual_keys = list(feats.keys())\n",
    "        semantic_to_actual = {\n",
    "            \"layer1\": actual_keys[0] if len(actual_keys)>0 else None,\n",
    "            \"layer2\": actual_keys[1] if len(actual_keys)>1 else None,\n",
    "            \"layer3\": actual_keys[2] if len(actual_keys)>2 else None,\n",
    "            \"layer4\": actual_keys[3] if len(actual_keys)>3 else None,\n",
    "        }\n",
    "        wanted = []\n",
    "        for lvl in selected_levels:\n",
    "            if lvl in actual_keys: wanted.append(lvl)\n",
    "            elif lvl in semantic_to_actual and semantic_to_actual[lvl] is not None: wanted.append(semantic_to_actual[lvl])\n",
    "        self.selected_actual = wanted\n",
    "\n",
    "        self.block_fpn_in = nn.ModuleDict()\n",
    "        for k in self.selected_actual:\n",
    "            C = feats[k].shape[1]\n",
    "            branches = (\"local\",\"global\") if config.get(\"BRANCH_PRESET\",\"none\") == \"local_global\" else ()\n",
    "            self.block_fpn_in[str(k)] = get_block(C, config.get(\"block_type\",\"none\"),\n",
    "                                                  branches, config.get(\"gating_type\",\"none\"))\n",
    "\n",
    "        first_out = next(iter(fpn_feats.keys()))\n",
    "        self.out_channels = fpn_feats[first_out].shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.body(x)\n",
    "        for k in self.selected_actual:\n",
    "            feats[k] = self.block_fpn_in[str(k)](feats[k])\n",
    "        return self.fpn(feats)\n",
    "\n",
    "def build_model_from_config(num_classes, insert_level, config, transform_img_size=None):\n",
    "    pretrained = retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1)\n",
    "    convert_bn_to_gn(pretrained.backbone.body, num_groups=32, convert_frozen=False)\n",
    "\n",
    "    sel_levels = [LEVEL_MAP[insert_level]] if config.get(\"block_type\",\"none\") != \"none\" else []\n",
    "    if sel_levels:\n",
    "        bb = CustomBackboneBlockBeforeFPN(pretrained.backbone, selected_levels=sel_levels, config=config)\n",
    "    else:\n",
    "        bb = pretrained.backbone\n",
    "\n",
    "    ag = make_anchor_generator_for(bb)\n",
    "    model = torchvision.models.detection.RetinaNet(\n",
    "        backbone=bb,\n",
    "        num_classes=num_classes,\n",
    "        anchor_generator=ag,\n",
    "        head=pretrained.head,\n",
    "        transform=pretrained.transform,\n",
    "        detections_per_img=100,\n",
    "        nms_thresh=0.5,\n",
    "        score_thresh=0.05,\n",
    "    )\n",
    "\n",
    "    # Update classification head to match class count\n",
    "    in_ch = model.head.classification_head.cls_logits.in_channels\n",
    "    n_anchors = model.head.classification_head.num_anchors\n",
    "    model.head.classification_head.cls_logits = nn.Conv2d(in_ch, n_anchors*num_classes, kernel_size=3, padding=1)\n",
    "    model.head.classification_head.num_classes = num_classes\n",
    "    torch.nn.init.normal_(model.head.classification_head.cls_logits.weight, std=0.01)\n",
    "    prior_prob = 0.01\n",
    "    bias_value = -torch.log(torch.tensor((1.0 - prior_prob) / prior_prob))\n",
    "    torch.nn.init.constant_(model.head.classification_head.cls_logits.bias, bias_value)\n",
    "\n",
    "\n",
    "    # Override detection transform size if requested\n",
    "    if transform_img_size is not None and hasattr(model, \"transform\"):\n",
    "        try:\n",
    "            # min_size expects a tuple/list of sizes for multi-scale. Use single fixed size.\n",
    "            model.transform.min_size = (int(transform_img_size),)\n",
    "            model.transform.max_size = int(transform_img_size)\n",
    "        except Exception:\n",
    "            pass\n",
    "    convert_bn_to_gn(model, num_groups=32, convert_frozen=False)\n",
    "    return model\n",
    "\n",
    "# ----------------------- Metrics: params, FLOPs, speed -------------------------\n",
    "\n",
    "def count_params_m(model):\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "# def try_flops_params(model, image_size=(3, 800, 1333)):\n",
    "#     params_m = count_params_m(model)\n",
    "#     # Build a CPU copy for FLOPs tools\n",
    "#     m = copy.deepcopy(model).to(\"cpu\").eval()\n",
    "#     dummy = torch.zeros(1,*image_size)\n",
    "#     # # correct: RetinaNet expects a list of [C,H,W] tensors\n",
    "#     # dummy = torch.zeros(*image_size) # 3xHxW?\n",
    "#     # fvcore\n",
    "#     if try_import(\"fvcore\"):\n",
    "#         try:\n",
    "#             from fvcore.nn import FlopCountAnalysis\n",
    "#             flops = FlopCountAnalysis(m, ([dummy],)).total()\n",
    "#             return {\"params_M\": float(params_m), \"FLOPs_G\": float(flops)/1e9}\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     # thop\n",
    "#     if try_import(\"thop\"):\n",
    "#         try:\n",
    "#             from thop import profile\n",
    "#             macs,_ = profile(m, inputs=([dummy],), verbose=False)\n",
    "#             return {\"params_M\": float(params_m), \"FLOPs_G\": float(macs)/1e9}\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     return {\"params_M\": float(params_m), \"FLOPs_G\": None}\n",
    "\n",
    "# def try_flops_params(model, image_size=(3, 640, 640)):\n",
    "#     \"\"\"\n",
    "#     Robust FLOPs:\n",
    "#       - counts parameters on the full model\n",
    "#       - counts FLOPs on a heads-only wrapper (backbone + FPN + heads),\n",
    "#         so there’s no transform/NMS/postprocess to confuse the counter.\n",
    "#       - uses fvcore first, falls back to thop\n",
    "#     \"\"\"\n",
    "#     params_m = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "#     class BackboneHead(nn.Module):\n",
    "#         def __init__(self, m: nn.Module):\n",
    "#             super().__init__()\n",
    "#             self.backbone = copy.deepcopy(m.backbone).cpu().eval()\n",
    "#             self.head = copy.deepcopy(m.head).cpu().eval()\n",
    "\n",
    "#         def forward(self, x: torch.Tensor):\n",
    "#             # x: [N,3,H,W], bypass torchvision's detection transform\n",
    "#             feats = self.backbone(x)\n",
    "#             if isinstance(feats, dict):\n",
    "#                 feats = list(feats.values())\n",
    "#             cls_logits, bbox_reg = self.head(feats)  # lists per FPN level\n",
    "#             # return a tuple so graph isn't pruned\n",
    "#             return tuple(cls_logits) + tuple(bbox_reg)\n",
    "\n",
    "#     bh = BackboneHead(model)\n",
    "#     dummy = torch.zeros(1, *image_size)  # batch=1 for FLOPs\n",
    "\n",
    "#     # Try fvcore\n",
    "#     try:\n",
    "#         from fvcore.nn import FlopCountAnalysis\n",
    "#         flops = FlopCountAnalysis(bh, dummy).total()\n",
    "#         return {\"params_M\": float(params_m), \"FLOPs_G\": float(flops) / 1e9}\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # Fallback: thop\n",
    "#     try:\n",
    "#         from thop import profile\n",
    "#         macs, _ = profile(bh, inputs=(dummy,), verbose=False)\n",
    "#         return {\"params_M\": float(params_m), \"FLOPs_G\": float(macs) / 1e9}\n",
    "#     except Exception:\n",
    "#         return {\"params_M\": float(params_m), \"FLOPs_G\": None}\n",
    "\n",
    "def try_flops_params(model, image_size=(3, 640, 640)):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        {\"params_M\": float, \"FLOPs_G\": float or None}\n",
    "\n",
    "    Strategy:\n",
    "      1) Heads-only wrapper (backbone + FPN + heads) with fvcore\n",
    "      2) Fallback to thop on the same wrapper\n",
    "      3) Final fallback: manual conv/linear FLOPs via forward hooks\n",
    "    \"\"\"\n",
    "    params_m = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "    class BackboneHead(nn.Module):\n",
    "        def __init__(self, m: nn.Module):\n",
    "            super().__init__()\n",
    "            self.backbone = copy.deepcopy(m.backbone).cpu().eval()\n",
    "            self.head = copy.deepcopy(m.head).cpu().eval()\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            # x: [N,3,H,W], bypass torchvision detection transform/postprocess\n",
    "            feats = self.backbone(x)\n",
    "            if isinstance(feats, dict):\n",
    "                feats = list(feats.values())\n",
    "            cls_logits, bbox_reg = self.head(feats)  # lists per FPN level\n",
    "            # return tuple to keep graph from collapsing\n",
    "            return tuple(cls_logits) + tuple(bbox_reg)\n",
    "\n",
    "    bh = BackboneHead(model)\n",
    "    dummy = torch.zeros(1, *image_size)\n",
    "\n",
    "    # 1) fvcore\n",
    "    try:\n",
    "        from fvcore.nn import FlopCountAnalysis\n",
    "        flops = FlopCountAnalysis(bh, dummy).total()\n",
    "        if flops and flops > 1e6:  # > ~1e6 FLOPs to avoid the “0.0G” trap\n",
    "            return {\"params_M\": float(params_m), \"FLOPs_G\": float(flops) / 1e9}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) thop\n",
    "    try:\n",
    "        from thop import profile\n",
    "        macs, _ = profile(bh, inputs=(dummy,), verbose=False)\n",
    "        if macs and macs > 1e6:\n",
    "            return {\"params_M\": float(params_m), \"FLOPs_G\": float(macs) / 1e9}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Manual conv/linear FLOPs via forward hooks (MACs*2)\n",
    "    def conv_linear_flops(m: nn.Module, inp_size=(1, 3, 640, 640)):\n",
    "        hooks, flops = [], []\n",
    "        def on_conv(mod, inputs, outputs):\n",
    "            # outputs: [N, Cout, H, W]\n",
    "            out = outputs\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                out = out[0]\n",
    "            N, Cout, H, W = out.shape\n",
    "            Cin = mod.in_channels\n",
    "            kH, kW = mod.kernel_size\n",
    "            groups = mod.groups\n",
    "            macs_per_out = (Cin // groups) * kH * kW\n",
    "            macs = N * Cout * H * W * macs_per_out\n",
    "            flops.append(2 * macs)\n",
    "        def on_linear(mod, inputs, outputs):\n",
    "            out = outputs\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                out = out[0]\n",
    "            N = out.shape[0] if out.dim() > 1 else 1\n",
    "            flops.append(2 * N * mod.in_features * mod.out_features)\n",
    "\n",
    "        for mod in m.modules():\n",
    "            if isinstance(mod, nn.Conv2d):\n",
    "                hooks.append(mod.register_forward_hook(on_conv))\n",
    "            elif isinstance(mod, nn.Linear):\n",
    "                hooks.append(mod.register_forward_hook(on_linear))\n",
    "        with torch.no_grad():\n",
    "            m(torch.zeros(*inp_size))\n",
    "        for h in hooks: h.remove()\n",
    "        return sum(flops)\n",
    "\n",
    "    manual = conv_linear_flops(bh, (1, *image_size))\n",
    "    return {\"params_M\": float(params_m), \"FLOPs_G\": float(manual) / 1e9 if manual else None}\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_inference_random(model, device, img_size=640, batch_size=4, warmup_batches=20, measure_images=200):\n",
    "    model.eval()\n",
    "    H = W = int(img_size)\n",
    "    processed = 0\n",
    "    timings = []\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(max(1, warmup_batches)):\n",
    "        images = [torch.rand(3,H,W, device=device) for _ in range(batch_size)]\n",
    "        _ = model(images)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        starter = torch.cuda.Event(enable_timing=True)\n",
    "        ender = torch.cuda.Event(enable_timing=True)\n",
    "        while processed < measure_images:\n",
    "            images = [torch.rand(3,H,W, device=device) for _ in range(batch_size)]\n",
    "            starter.record(); _ = model(images); ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            ms = starter.elapsed_time(ender) / max(len(images),1)\n",
    "            timings.append(ms)\n",
    "            processed += len(images)\n",
    "    else:\n",
    "        while processed < measure_images:\n",
    "            images = [torch.rand(3,H,W, device=device) for _ in range(batch_size)]\n",
    "            t0 = time.perf_counter(); _ = model(images); dt = time.perf_counter() - t0\n",
    "            timings.append(1000.0*dt / max(len(images),1))\n",
    "            processed += len(images)\n",
    "\n",
    "    lat = float(np.mean(timings)) if len(timings) else float(\"nan\")\n",
    "    ips = 1000.0/lat if lat > 0 else float(\"nan\")\n",
    "    return {\"latency_ms_per_image\": lat, \"images_per_second\": ips}\n",
    "\n",
    "# ----------------------- Checkpoint handling ----------------------------------\n",
    "\n",
    "def infer_insert_level_from_name(name):\n",
    "    m = re.search(r'\\b(C3|C4|C5)\\b', name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def infer_mode_from_config(cfg):\n",
    "    bt = cfg.get(\"block_type\", \"none\")\n",
    "    if bt == \"none\": return \"baseline\"\n",
    "    if bt == \"se\": return \"se\"\n",
    "    if bt == \"cbam\": return \"cbam\"\n",
    "    if bt == \"lgf\":\n",
    "        gt = cfg.get(\"gating_type\",\"sum\")\n",
    "        return f\"lgf_{gt}\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def num_classes_from_ckpt_state(sd, n_anchors):\n",
    "    # Read classification logits weight shape\n",
    "    key = \"head.classification_head.cls_logits.weight\"\n",
    "    if key not in sd:\n",
    "        # sometimes saved under 'ema_state_dict' with prefix removed already\n",
    "        for k in sd.keys():\n",
    "            if k.endswith(\"cls_logits.weight\"):\n",
    "                key = k\n",
    "                break\n",
    "    n_out = sd[key].shape[0]\n",
    "    return int(n_out // n_anchors)\n",
    "\n",
    "def load_checkpoint_build_model(ckpt_path, insert_level_override=None, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", transform_img_size=None):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    sd = ckpt.get(\"ema_state_dict\") or ckpt.get(\"model_state_dict\") or ckpt\n",
    "\n",
    "    cfg = ckpt.get(\"config\", {})  # contains block_type/gating_type/BRANCH_PRESET\n",
    "    exp_name = ckpt.get(\"experiment_name\", os.path.basename(ckpt_path))\n",
    "\n",
    "    ins = insert_level_override or infer_insert_level_from_name(exp_name) or \"C3\"\n",
    "    # Build a temp model to read n_anchors, then compute num_classes, then rebuild\n",
    "    tmp_model = build_model_from_config(num_classes=80, insert_level=ins, config=cfg, transform_img_size=transform_img_size).to(device)\n",
    "    n_anchors = tmp_model.head.classification_head.num_anchors\n",
    "    num_classes = num_classes_from_ckpt_state(sd, n_anchors)\n",
    "\n",
    "    model = build_model_from_config(num_classes=num_classes, insert_level=ins, config=cfg, transform_img_size=transform_img_size).to(device)\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if unexpected:\n",
    "        print(f\"[WARN] Unexpected keys in state_dict: {unexpected}\")\n",
    "    if missing:\n",
    "        # Common harmless misses: num_batches_tracked, etc.\n",
    "        # Only warn if many keys are missing\n",
    "        miss_sig = [k for k in missing if \"num_batches_tracked\" not in k]\n",
    "        if miss_sig:\n",
    "            print(f\"[WARN] Missing keys in state_dict: {miss_sig[:5]}{' ...' if len(miss_sig)>5 else ''}\")\n",
    "\n",
    "    mode = infer_mode_from_config(cfg)\n",
    "    return model, {\"mode\": mode, \"insert_level\": ins, \"num_classes\": num_classes, \"experiment_name\": exp_name}\n",
    "\n",
    "# ------------------------------ CLI -------------------------------------------\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(\"Measure params, FLOPs, and throughput for saved RetinaNet checkpoints\")\n",
    "    p.add_argument(\"--ckpts\", type=str, nargs=\"+\", required=True, help=\"Path(s) or glob(s) to .pth checkpoints\")\n",
    "    p.add_argument(\"--insert-level\", type=str, choices=[\"C3\",\"C4\",\"C5\"], default=None, help=\"Override insert level if not in experiment name\")\n",
    "    p.add_argument(\"--img-size\", type=int, default=640, help=\"Square image size for FLOPs and timing\")\n",
    "    p.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size for timing\")\n",
    "    p.add_argument(\"--warmup-batches\", type=int, default=20, help=\"Warmup batches before timing\")\n",
    "    p.add_argument(\"--measure-images\", type=int, default=200, help=\"Number of images to time in total\")\n",
    "    p.add_argument(\"--device\", type=str, default=\"auto\", choices=[\"auto\",\"cuda\",\"cpu\"])\n",
    "    p.add_argument(\"--report-csv\", type=str, default=None, help=\"Optional CSV to write results\")\n",
    "    return p.parse_args()\n",
    "\n",
    "def expand_paths(paths):\n",
    "    expanded = []\n",
    "    for p in paths:\n",
    "        if any(ch in p for ch in [\"*\",\"?\",\"[\"]):\n",
    "            expanded.extend(glob(p))\n",
    "        else:\n",
    "            expanded.append(p)\n",
    "    # de-dup while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in expanded:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    set_torch_determinism()\n",
    "\n",
    "    device = torch.device(\"cuda\" if (args.device==\"auto\" and torch.cuda.is_available()) or args.device==\"cuda\" else \"cpu\")\n",
    "    torch.set_float32_matmul_precision(\"high\") if device.type==\"cuda\" and hasattr(torch, \"set_float32_matmul_precision\") else None\n",
    "\n",
    "    ckpt_paths = expand_paths(args.ckpts)\n",
    "    if not ckpt_paths:\n",
    "        raise SystemExit(\"No checkpoints matched the given patterns.\")\n",
    "\n",
    "    rows = []\n",
    "    for ck in ckpt_paths:\n",
    "        model, meta = load_checkpoint_build_model(ck, insert_level_override=args.insert_level, device=device, transform_img_size=args.img_size)\n",
    "        # Params + FLOPs\n",
    "        flp = try_flops_params(copy.deepcopy(model).to(\"cpu\").eval(), image_size=(3, args.img_size, args.img_size))\n",
    "        # Speed\n",
    "        spd = benchmark_inference_random(model, device=device, img_size=args.img_size,\n",
    "                                         batch_size=args.batch_size, warmup_batches=args.warmup_batches,\n",
    "                                         measure_images=args.measure_images)\n",
    "\n",
    "        row = {\n",
    "            \"checkpoint\": ck,\n",
    "            \"experiment\": meta.get(\"experiment_name\",\"\"),\n",
    "            \"mode\": meta[\"mode\"],\n",
    "            \"insert_level\": meta[\"insert_level\"],\n",
    "            \"num_classes\": meta[\"num_classes\"],\n",
    "            \"img_size\": args.img_size,\n",
    "            \"params_M\": round(flp[\"params_M\"], 3),\n",
    "            \"FLOPs_G\": None if flp[\"FLOPs_G\"] is None else round(flp[\"FLOPs_G\"], 3),\n",
    "            \"latency_ms_per_image\": round(spd[\"latency_ms_per_image\"], 3),\n",
    "            \"images_per_second\": round(spd[\"images_per_second\"], 3),\n",
    "            \"device\": str(device),\n",
    "            \"batch_size\": args.batch_size,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        # Print one-line summary\n",
    "        print(f\"[OK] {os.path.basename(ck)} | {row['mode']} {row['insert_level']} | \"\n",
    "              f\"Params {row['params_M']}M | FLOPs {row['FLOPs_G']}G | \"\n",
    "              f\"{row['images_per_second']} img/s @ {args.img_size} on {device}\")\n",
    "\n",
    "    # Optional CSV\n",
    "    if args.report_csv:\n",
    "        os.makedirs(os.path.dirname(args.report_csv) or \".\", exist_ok=True)\n",
    "        with open(args.report_csv, \"w\", newline=\"\") as f:\n",
    "            wr = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "            wr.writeheader()\n",
    "            wr.writerows(rows)\n",
    "        print(f\"[WROTE] {args.report_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_11.6_env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
